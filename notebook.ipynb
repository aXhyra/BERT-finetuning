{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Fine_Tuning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets\n",
    "!apt install git-lfs\n",
    "!git lfs install\n",
    "!pip install optuna\n",
    "!pip install wandb"
   ],
   "metadata": {
    "id": "1SaYsilNuF-g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub.hf_api import HfApi, HfFolder\n",
    "import subprocess\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import requests\n",
    "import wandb\n",
    "\n",
    "from huggingface_hub.constants import ENDPOINT\n",
    "USERNAME_PLACEHOLDER = \"hf_user\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ],
   "metadata": {
    "id": "5WWBp9BD-CM0"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Huggingface login support functions"
   ],
   "metadata": {
    "id": "F2VXAimBHHGJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def write_to_credential_store(username: str, password: str):\n",
    "    with subprocess.Popen(\n",
    "        \"git credential-store store\".split(),\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    ) as process:\n",
    "        input_username = f\"username={username.lower()}\"\n",
    "        input_password = f\"password={password}\"\n",
    "\n",
    "        process.stdin.write(\n",
    "            f\"url={ENDPOINT}\\n{input_username}\\n{input_password}\\n\\n\".encode(\"utf-8\")\n",
    "        )\n",
    "        process.stdin.flush()\n",
    "\n",
    "\n",
    "def hf_login(hf_api, username=None, password=None, token=None):\n",
    "    if token is not None:\n",
    "        write_to_credential_store(USERNAME_PLACEHOLDER, token)\n",
    "        HfFolder.save_token(token)\n",
    "        print(\"Login successful\")\n",
    "        print(\"Your token has been saved to\", HfFolder.path_token)\n",
    "        helpers = currently_setup_credential_helpers()\n",
    "\n",
    "def currently_setup_credential_helpers(directory=None):\n",
    "    try:\n",
    "        output = subprocess.run(\n",
    "            \"git config --list\".split(),\n",
    "            stderr=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            encoding=\"utf-8\",\n",
    "            check=True,\n",
    "            cwd=directory,\n",
    "        ).stdout.split(\"\\n\")\n",
    "\n",
    "        current_credential_helpers = []\n",
    "        for line in output:\n",
    "            if \"credential.helper\" in line:\n",
    "                current_credential_helpers.append(line.split(\"=\")[-1])\n",
    "    except subprocess.CalledProcessError as exc:\n",
    "        raise EnvironmentError(exc.stderr)\n",
    "\n",
    "    return current_credential_helpers\n"
   ],
   "metadata": {
    "id": "mTBAbqanGQGb"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {
    "id": "KZOOezAFpvfs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Huggingface and wandb login"
   ],
   "metadata": {
    "id": "N2b79dvDGzNu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "hf_login(HfApi(), token=\"\")\n",
    "\n",
    "wandb.login(key=\"\")\n",
    "wandb.init(project=\"ATML\", entity=\"aXhyra\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "0DNMk_UoGr93",
    "outputId": "1bb35393-6c6a-4b7e-fb6e-f45d1eb5b0fe"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Login successful\n",
      "Your token has been saved to /root/.huggingface/token\n",
      "Authenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpietrotrope\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/pietrotrope/ATML/runs/okxw7uw9\" target=\"_blank\">treasured-wave-1</a></strong> to <a href=\"https://wandb.ai/pietrotrope/ATML\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc461106390>"
      ],
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/pietrotrope/ATML/runs/okxw7uw9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()\n",
    "%env WANDB_PROJECT=tweeteval_sentiment\n",
    "%env WANDB_LOG_MODEL=true"
   ],
   "metadata": {
    "id": "E8G-nQFiNi2O"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Dataset:\n",
    "  dataset = None\n",
    "  tokenized_dataset = None\n",
    "\n",
    "  def __init__(self, task, tokenizer_name, s_key1=\"text\", s_key2 = None):\n",
    "    self.dataset_name = \"tweet_eval\"\n",
    "    self.task = task\n",
    "    self.s_key1 = s_key1\n",
    "    self.s_key2 = s_key2\n",
    "    self.tokenizer_name = tokenizer_name\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "    self.dataset = load_dataset(self.dataset_name, task)\n",
    "    self.tokenized_dataset = self.dataset.map(self.preprocess_function, batched=True)\n",
    "    self.n_classes = np.max(self.dataset[\"validation\"][\"label\"]) + 1\n",
    "    self.retrieve_labels()\n",
    "  \n",
    "  def retrieve_labels(self):\n",
    "    self.labels = {}\n",
    "\n",
    "    r = requests.get(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/\"+self.task+\"/mapping.txt\")\n",
    "    tmp = r.text.split(\"\\n\")\n",
    "    for el in tmp:\n",
    "      if len(el) > 1:\n",
    "        tmp2 = el.split(\"\\t\")\n",
    "        self.labels[tmp2[0]] = tmp2[1]\n",
    "\n",
    "  def preprocess_function(self, dataset):\n",
    "    if self.s_key2 is None:\n",
    "        return self.tokenizer(dataset[self.s_key1], truncation=True)\n",
    "    return self.tokenizer(dataset[self.s_key1], dataset[self.s_key2], truncation=True)"
   ],
   "metadata": {
    "id": "0xRbG38fD0MF"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pF2EJ9xmqe_u"
   },
   "outputs": [],
   "source": [
    "class Engine:\n",
    "\n",
    "  model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "  @staticmethod\n",
    "  def compute_metrics(eval_pred):\n",
    "      metric = load_metric(\"f1\")\n",
    "      predictions, labels = eval_pred\n",
    "      predictions = predictions.argmax(axis=-1)\n",
    "      return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "  @staticmethod\n",
    "  def my_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64]),\n",
    "    }\n",
    "\n",
    "  def __init__(self, data, args, device=\"cuda:0\"):\n",
    "    self.model = None \n",
    "    self.args = args\n",
    "    self.trainer = None\n",
    "    self.best_run = None\n",
    "    self.dataset = data\n",
    "    self.results = None\n",
    "    self.device = device\n",
    "\n",
    "  def model_init(self):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(Engine.model_checkpoint, id2label = self.dataset.labels, num_labels=self.dataset.n_classes, return_dict=True)\n",
    "\n",
    "  def load_trainer(self, use_init=False):\n",
    "    if use_init:\n",
    "      self.trainer = Trainer(\n",
    "        model_init = self.model_init,\n",
    "        args=self.args,\n",
    "        train_dataset=self.dataset.tokenized_dataset['train'],\n",
    "        eval_dataset=self.dataset.tokenized_dataset['validation'],\n",
    "        tokenizer = self.dataset.tokenizer,\n",
    "        compute_metrics = Engine.compute_metrics,\n",
    "      )\n",
    "    else:\n",
    "      self.trainer = Trainer(\n",
    "        model = self.model,\n",
    "        args=self.args,\n",
    "        train_dataset=self.dataset.tokenized_dataset['train'],\n",
    "        eval_dataset=self.dataset.tokenized_dataset['validation'],\n",
    "        tokenizer = self.dataset.tokenizer,\n",
    "        compute_metrics = Engine.compute_metrics,\n",
    "      )\n",
    "    \n",
    "  def load_train_args(self, opt_name = \"test\", lr = 2e-5, epochs=4, batch_size=16, push_to_hub=False, seed=0):\n",
    "    self.args = TrainingArguments(\n",
    "      opt_name,\n",
    "      seed = seed,\n",
    "      evaluation_strategy = \"epoch\",\n",
    "      save_strategy = \"epoch\",\n",
    "      learning_rate=lr,\n",
    "      per_device_train_batch_size=batch_size,\n",
    "      per_device_eval_batch_size=batch_size,\n",
    "      num_train_epochs=epochs,\n",
    "      weight_decay=0.01,\n",
    "      load_best_model_at_end=True,\n",
    "      push_to_hub = push_to_hub,\n",
    "      metric_for_best_model=\"f1\",\n",
    "      report_to=\"wandb\",\n",
    "      save_total_limit=1,\n",
    "      run_name=opt_name,\n",
    "    )\n",
    "\n",
    "\n",
    "  def train(self, epochs, seed=0, opt_name=\"test\"):\n",
    "    self.load_train_args(opt_name,\n",
    "                         self.best_run.hyperparameters[\"learning_rate\"],\n",
    "                         epochs,\n",
    "                         self.best_run.hyperparameters[\"per_device_train_batch_size\"],\n",
    "                         True, seed)\n",
    "    self.load_trainer(True)\n",
    "    \n",
    "    self.results = self.trainer.train()\n",
    "    self.trainer.push_to_hub()\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "  def evaluate(self):\n",
    "    if self.trainer is None:\n",
    "      print(\"[!] Training required\")\n",
    "      return\n",
    "    self.trainer.evaluate()\n",
    "\n",
    "\n",
    "  def hyperparameter_search(self, n_trials = 5):\n",
    "    if self.args is None:\n",
    "      print(\"[!] TraininArgument object is required\")\n",
    "      return -1\n",
    "    self.load_trainer(True)\n",
    "    self.best_run = self.trainer.hyperparameter_search(n_trials=n_trials, direction=\"maximize\", hp_space=self.my_hp_space)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "  def load_model(self, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    self.dataset.tokenizer = tokenizer\n",
    "    self.model.to(self.device)\n",
    "\n",
    "\n",
    "  def predict(self, input_text):    \n",
    "    input_text_tokenized = self.dataset.tokenizer(input_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    if self.device is not None:\n",
    "      input_text_tokenized.to(self.device)\n",
    "    prediction = self.model(**input_text_tokenized).logits\n",
    "\n",
    "    result = torch.softmax(prediction, dim=1)\n",
    "    result = np.argmax(result.tolist())\n",
    "\n",
    "    return self.dataset.labels[str(result)]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "task = \"sentiment\"\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 16\n",
    "metric_name = \"f1\"\n",
    "\n",
    "# for task in tasks:\n",
    "\n",
    "name = f\"{task}\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "  name,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  save_strategy = \"no\",\n",
    "  learning_rate=2e-5,\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  per_device_eval_batch_size=batch_size,\n",
    "  num_train_epochs=4,\n",
    "  weight_decay=0.01,\n",
    "  load_best_model_at_end=False,\n",
    "  push_to_hub = False,\n",
    "  metric_for_best_model=metric_name,\n",
    "  report_to=\"wandb\",\n",
    "  save_total_limit=1,\n",
    "  run_name=name\n",
    ")\n",
    "\n",
    "dataset = Dataset(task, \"distilbert-base-uncased\")\n",
    "engine = Engine(dataset, args)\n",
    "\n",
    "engine.hyperparameter_search(10)\n",
    "\n",
    "name = f\"{task}_trained\"\n",
    "print(f\"\\n\\n [+] Training model: {name}\")\n",
    "engine.train(4, seed=0, opt_name=name)\n",
    "# os.system(f\"rm -rf {name}\")\n"
   ],
   "metadata": {
    "id": "DG4n55tlhiXc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49AzPtGtoFVV",
    "outputId": "cda6818f-7646-4710-cb6a-9615cec3bada"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fri Dec 10 09:15:42 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "engine.predict(\"I'm so happy you have cancer\")"
   ],
   "metadata": {
    "id": "eDj-lf7PdzqX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "outputId": "5993d161-87be-4691-bc8c-00f4b44c5630"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'positive'"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ]
  }
 ]
}